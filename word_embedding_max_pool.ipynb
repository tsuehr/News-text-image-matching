{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"word_embedding_max_pool.ipynb","provenance":[],"collapsed_sections":["ZLTdTqO9Go4b","cDDXflU0AGlv","woHKSzT74Jy0"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"VKL4JskCXwam"},"source":["#Please visit the word_embedding_linear.ipynb notebook for comments about paths and options"]},{"cell_type":"code","metadata":{"id":"5dSRJQjle-CC"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.nn.init\n","import torchvision.models as models\n","from torch.autograd import Variable\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","import torch.backends.cudnn as cudnn\n","from torch.nn.utils import clip_grad_norm\n","import pickle\n","import numpy as np\n","#!pip install \"nltk==3.4.5\"\n","from nltk.stem.cistem import Cistem\n","import nltk\n","#nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","stopwords_german = stopwords.words('german')\n","\n","#!pip install fasttext\n","import fasttext\n","from random import random\n","\n","import pandas as pd\n","from PIL import Image\n","import torchvision.transforms as transforms\n","from torch.utils import data\n","from torch.utils.data import Dataset, TensorDataset\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cdfDZqLulyfT","executionInfo":{"status":"ok","timestamp":1627392568479,"user_tz":-120,"elapsed":4985,"user":{"displayName":"Tom Sühr","photoUrl":"","userId":"12788046760778604768"}},"outputId":"79f6cb06-a43f-48bf-9b2b-464662d6026f"},"source":["#text_emb_model = fasttext.load_model('path/to/wiki.de.bin')\n","text_emb_model = fasttext.load_model('path/to/newspaper.bin')\n","\n","#text_emb_model = fasttext.train_unsupervised('/content/drive/MyDrive/Code/PJDS/custom_MAXHAL/mediaeval_vocab.npy', minn=3, maxn=5, dim=300)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"YSV7nQA9kI80"},"source":["\n","def l2norm(X):\n","    norm = torch.pow(X, 2).sum(dim=1, keepdim=True).sqrt()\n","    X = torch.div(X, norm)\n","    return X\n","\n","def cosine_sim(im, s):\n","    return im.mm(s.t())\n","\n","def padding(max_length, word_list):\n","  word_list = [w.lower() for w in word_list]\n","  if len(word_list)>=max_length:\n","    return word_list[:max_length]\n","  else:\n","    difference = max_length - len(word_list)\n","    word_list = word_list + difference*[\"\"]\n","    return word_list\n","\n","def create_n_grams(n,word):\n","  if len(word)<=n:\n","    return [word]\n","  else:\n","    output = [word[i:n+i] for i in range(0,len(word)-n+1)]\n","    return output\n","\n","def embed_with_3_4_5_grams(emb_model, word):\n","  three_grams = create_n_grams(3,word)\n","  four_grams = create_n_grams(4,word)\n","  five_grams = create_n_grams(5,word)\n","  total = [word] + three_grams + four_grams +five_grams\n","  total = [torch.from_numpy(emb_model[sw]).float()  for sw in total]\n","  total = torch.stack(total)\n","  total = torch.mean(total,0)\n","  return torch.unsqueeze(total,0)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"80iJo6kM7TWg"},"source":["#Model"]},{"cell_type":"code","metadata":{"id":"WW3Mb5zi7yPJ"},"source":["class MAXHAL(object):##Model\n","  def __init__(self, learning_rate):\n","    self.img_enc = ImageEncoder()\n","    self.txt_enc = TextEncoder()\n","    if torch.cuda.is_available():\n","      self.img_enc.cuda()\n","      self.txt_enc.cuda()\n","\n","    # Loss and Optimizer\n","    self.criterion = ContrastiveLoss()\n","    params = list(self.txt_enc.parameters())\n","    params += list(self.img_enc.fc.parameters())\n","\n","    self.params = params\n","\n","    self.optimizer = torch.optim.Adam(params, lr=learning_rate)\n","\n","  def train_start(self):\n","    self.img_enc.train()\n","    self.txt_enc.train()\n","\n","  def val_start(self):\n","    self.img_enc.eval()\n","    self.txt_enc.eval()\n","  \n","  def forward_emb(self, images, article,title,category):\n","    \n","    article = article.cuda()\n","    title = title.cuda()\n","    category = category.cuda()\n","\n","    images = torch.Tensor(images).cuda()\n","    \n","    img_emb = self.img_enc(images)\n","    txt_emb = self.txt_enc(article,title,category)\n","\n","    return img_emb, txt_emb\n","\n","  def forward_loss(self, img_emb, txt_emb, indices):\n","    loss, loss_text, loss_img = self.criterion(img_emb, txt_emb)\n","    return loss\n","\n","  def train_emb(self, images, article,title,category, ids):\n","    img_emb, txt_emb = self.forward_emb(images,article,title,category)\n","\n","    self.optimizer.zero_grad()\n","    loss = self.forward_loss(img_emb, txt_emb, ids)\n","    print(loss)\n","    loss.backward()\n","    self.optimizer.step()\n","\n","    del img_emb\n","    del txt_emb"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V1joUjXSCOHO"},"source":["class TextEncoder(nn.Module):\n","  def __init__(self):\n","    super(TextEncoder, self).__init__()\n","    self.word_dim = 300\n","    \n","    self.title_dim = 5\n","    self.article_dim = 25\n","    self.category_dim = 1\n","\n","    self.embed_size = 512\n","    self._pool = nn.AdaptiveMaxPool1d(1)\n","    self.fuse_pool = nn.AdaptiveMaxPool1d(1)\n","        \n","    self.fc = nn.Linear(300,512)\n","    #self.init_weights()\n","    \n","\n","  def forward(self,article,title,category):#input is fasttext embedding\n","\n","      out_art = self._pool(article).squeeze(2)\n","      out_tit = self._pool(title).squeeze(2)\n","      out_cat = self._pool(category).squeeze(2)\n","        \n","      out_fused = torch.stack([out_art,out_tit,out_cat]).transpose(0,1).transpose(1,2)\n","      out = self.fuse_pool(out_fused).squeeze(2)\n","      out = self.fc(out)\n","      out = l2norm(out)\n","      \n","      return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qNT7na3rI3GS"},"source":["class ImageEncoder(nn.Module):\n","\n","    def __init__(self, embed_size=512, finetune=False, cnn_type='vgg19',\n","                 use_abs=False, no_imgnorm=False):\n","        \"\"\"Load pretrained VGG19 and replace top fc layer.\"\"\"\n","        super(ImageEncoder, self).__init__()\n","        self.embed_size = embed_size\n","        self.no_imgnorm = no_imgnorm\n","        self.use_abs = use_abs\n","\n","        # Load a pre-trained model\n","        self.cnn = self.get_cnn(cnn_type, True)\n","\n","        # For efficient memory usage.\n","        for param in self.cnn.parameters():\n","            param.requires_grad = finetune\n","\n","        # Replace the last fully connected layer of CNN with a new one\n","        self.fc = nn.Linear(self.cnn.classifier._modules['6'].in_features,\n","                                embed_size)\n","        self.cnn.classifier = nn.Sequential(\n","            *list(self.cnn.classifier.children())[:-1])\n","\n","        self.init_weights()\n","\n","    def get_cnn(self, arch, pretrained):\n","        \"\"\"Load a pretrained CNN and parallelize over GPUs\n","        \"\"\"\n","        print(\"=> using pre-trained model '{}'\".format(arch))\n","        model = models.__dict__[arch](pretrained=True)\n","        model.features = nn.DataParallel(model.features)\n","        model.cuda()\n","        \n","        return model\n","\n","\n","    def init_weights(self):\n","        \"\"\"Xavier initialization for the fully connected layer\n","        \"\"\"\n","        r = np.sqrt(6.) / np.sqrt(self.fc.in_features +\n","                                  self.fc.out_features)\n","        self.fc.weight.data.uniform_(-r, r)\n","        self.fc.bias.data.fill_(0)\n","\n","    def forward(self, images):\n","        \"\"\"Extract image feature vectors.\"\"\"\n","        features = self.cnn(images).cuda()\n","\n","        # normalization in the image embedding space\n","        features = l2norm(features)\n","\n","        # linear projection to the joint embedding space\n","        features = self.fc(features).cuda()\n","\n","        # normalization in the joint embedding space\n","        if not self.no_imgnorm:\n","            features = l2norm(features)\n","\n","        # take the absolute value of the embedding (used in order embeddings)\n","        if self.use_abs:\n","            features = torch.abs(features)\n","\n","        return features"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4cX3RFypORKt"},"source":["class ContrastiveLoss(nn.Module):\n","  def __init__(self, margin=0.4):\n","    super(ContrastiveLoss, self).__init__()\n","    self.margin = margin\n","    self.sim = cosine_sim\n","    self.max_violation = False\n","    self.sim_power = 1.5\n","\n","  def forward(self, image, text):\n","    scores = self.sim(image, text)\n","    diagonal = scores.diag().view(image.size(0), 1)\n","\n","    d1 = diagonal.expand_as(scores)\n","    d2 = diagonal.t().expand_as(scores)\n","\n","    cost_text = (self.margin + scores - d1).clamp(min=0)\n","    cost_im = (self.margin + scores - d2).clamp(min=0)\n","\n","    mask = torch.eye(scores.size(0)) > 0.5\n","    I = Variable(mask)\n","    if torch.cuda.is_available():\n","      I = I.cuda()\n","    cost_text = cost_text.masked_fill_(I, 0)\n","    cost_im = cost_im.masked_fill_(I, 0)\n","    \n","    cost_text = torch.pow(cost_text, self.sim_power)\n","    cost_im = torch.pow(cost_im, self.sim_power)\n","\n","    return cost_text.sum() + cost_im.sum(), cost_text.sum(), cost_im.sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PiZdbTCc3ukG"},"source":["class ArticleDataset(data.Dataset):\n","    def __init__(self,dataset,text_emb_model, split):\n","      \n","      self.basepath = 'path/to/images'\n","      \n","      normalizer = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                   std=[0.229, 0.224, 0.225])\n","      if split=='test':\n","        t_list = [transforms.Resize(250), transforms.CenterCrop(250)]\n","      else:\n","        t_list = [transforms.Resize(250), transforms.CenterCrop(250),transforms.RandomResizedCrop(250), transforms.RandomHorizontalFlip()]\n","      t_end = [transforms.ToTensor(), normalizer]\n","      \n","      t_end = [transforms.ToTensor(), normalizer]\n","      \n","      self.stemmer = Cistem()\n","      self.transform = transforms.Compose(t_list + t_end)\n","      self.dataset = dataset \n","      self.text_emb_model =  text_emb_model\n","        \n","    def __getitem__(self, index):\n","      row = self.dataset.iloc[index]\n","      path = row[['imgFile']].tolist()[0]              \n","      image = Image.open(self.basepath + '/'+path).convert('RGB')\n","      image = self.transform(image)\n","      \n","      title = row[['title']].tolist()[0]\n","      article = row[['text']].tolist()[0]\n","      category = row[['category']].tolist()[0]\n","      title = title.split(' ')\n","      article = article.split(' ')\n","      category = category.split(' ')\n","      \n","      title = [self.stemmer.stem(w) for w in title]\n","      article = [self.stemmer.stem(w) for w in article]\n","      category = [self.stemmer.stem(w) for w in category]\n","\n","      title = padding(5,title)\n","      article = padding(25,article)\n","      category = padding(1,category)\n","\n","      title = [embed_with_3_4_5_grams(self.text_emb_model,t) for t in title]\n","      title = torch.stack(title)\n","      article = [embed_with_3_4_5_grams(self.text_emb_model,t) for t in article]\n","      article = torch.stack(article)\n","      category = [embed_with_3_4_5_grams(self.text_emb_model,t) for t in category]\n","      category = torch.stack(category)\n","      \n","      title = title.t()\n","      article = article.t()\n","      category = category.t()\n","\n","      return image, article, title, category, index\n","\n","    def __len__(self):\n","      return self.dataset.shape[0]\n","\n","class TextMediaeval(data.Dataset):\n","    def __init__(self,dataset,text_emb_model):\n","      \n","      self.dataset = dataset \n","      self.text_emb_model =  text_emb_model\n","      self.stemmer = Cistem()\n","      \n","    def __getitem__(self, index):\n","      row = self.dataset.iloc[index]\n","\n","      title = row[['title']].tolist()[0]\n","      article = row[['text']].tolist()[0]\n","      category = row[['category']].tolist()[0]\n","      title = title.split(' ')\n","      article = article.split(' ')\n","      category = category.split(' ')\n","      \n","      title = [self.stemmer.stem(w) for w in title]\n","      article = [self.stemmer.stem(w) for w in article]\n","      category = [self.stemmer.stem(w) for w in category]\n","\n","      title = padding(5,title)\n","      article = padding(25,article)\n","      category = padding(1,category)\n","\n","      title = [embed_with_3_4_5_grams(self.text_emb_model,t) for t in title]\n","      title = torch.stack(title)\n","      article = [embed_with_3_4_5_grams(self.text_emb_model,t) for t in article]\n","      article = torch.stack(article)\n","      category = [embed_with_3_4_5_grams(self.text_emb_model,t) for t in category]\n","      category = torch.stack(category)\n","      \n","      title = title.t()\n","      article = article.t()\n","      category = category.t()\n","      \n","      return article, title, category\n","\n","    def __len__(self):\n","      return self.dataset.shape[0]\n","\n","class ImageMediaeval(data.Dataset):\n","    def __init__(self):\n","      \n","      self.basepath = '/path/to/image/test/'\n","      \n","      normalizer = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                   std=[0.229, 0.224, 0.225])\n","      t_list = []\n","      t_list = [transforms.Resize(250), transforms.CenterCrop(250)]\n","      t_end = [transforms.ToTensor(), normalizer]\n","      \n","      self.transform = transforms.Compose(t_list + t_end)\n","      self.dataset = os.listdir(self.basepath)\n","        \n","    def __getitem__(self, index):\n","      file_name = self.dataset[index]\n","                    \n","      image = Image.open(self.basepath + file_name).convert('RGB')\n","      image = self.transform(image)\n","      \n","      return image, index\n","\n","    def __len__(self):\n","      return len(self.dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bqjnow2r7J0R"},"source":["#Validation"]},{"cell_type":"code","metadata":{"id":"-LEDnpd5tJyb"},"source":["def validate(val_loader, model, text='', metrics_only=True):\n","\n","    model.val_start()\n","\n","    # compute the encoding for all the validation images and captions\n","    img_embs, cap_embs = encode_data(model, val_loader)\n","\n","    if metrics_only:\n","      return t2i_article_metrics(img_embs, cap_embs)\n","    else:\n","      return t2i_article_indexes(img_embs, cap_embs)\n","\n","def encode_data(model, data_loader, log_step=10, logging=print):\n","    \"\"\"Encode all images and captions loadable by `data_loader`\n","    \"\"\"\n","\n","    # switch to evaluate mode\n","    model.val_start()\n","\n","    # numpy array to keep all the embeddings\n","    img_embs = None\n","    cap_embs = None\n","    print(\"data_loader.dataset length:\", len(data_loader.dataset))\n","    for i, (images, article,title,category, ids) in enumerate(data_loader):\n","        # compute the embeddings\n","        img_emb, cap_emb = model.forward_emb(images, article,title,category)\n","\n","        if img_embs is None:\n","          img_embs = np.zeros((len(data_loader.dataset), img_emb.size(1)))\n","          cap_embs = np.zeros((len(data_loader.dataset), cap_emb.size(1)))\n","\n","        # preserve the embeddings by copying from gpu and converting to numpy\n","        img_embs[ids] = img_emb.data.cpu().numpy().copy()\n","        cap_embs[ids] = cap_emb.data.cpu().numpy().copy()\n","\n","        del images, article,title,category\n","\n","    return img_embs, cap_embs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fz_MWMmlO7DL"},"source":["def t2i_article_indexes(images, captions, npts=None, measure='cosine', return_ranks=False):\n","    \"\"\"\n","    Text->Images (Image Search)\n","    Images: (N, K) matrix of images\n","    Captions: (N, K) matrix of captions\n","    \"\"\"\n","    if npts is None:\n","        npts = captions.shape[0]\n","\n","    ims = images\n","\n","    total_score = .0\n","    counter = 0\n","    total_index = []\n","    for index in range(npts):\n","\n","        # Get query captions\n","        queries = captions[index:index+1]\n","\n","        # Compute scores\n","        d = np.dot(queries, ims.T)\n","        inds = np.zeros(d.shape)\n","        \n","        for i in range(len(inds)):\n","          total_index.append(np.argsort(d[i])[::-1][:100])\n","\n","    return total_index\n","\n","def t2i_article_metrics(images, captions, npts=None, measure='cosine', return_ranks=False):\n","    \"\"\"\n","    Text->Images (Image Search)\n","    Images: (N, K) matrix of images\n","    Captions: (N, K) matrix of captions\n","    \"\"\"\n","    if npts is None:\n","        npts = captions.shape[0]\n","    print(npts)\n","    ims = images\n","\n","    ranks = np.zeros(npts)\n","    top1 = np.zeros(npts)\n","    \n","    total_score = .0\n","    counter = 0\n","    total_index = []\n","    for index in range(npts):\n","\n","        # Get query captions\n","        queries = captions[index:index+1]\n","\n","        # Compute scores\n","        d = np.dot(queries, ims.T)\n","        inds = np.zeros(d.shape)\n","        \n","        for i in range(len(inds)):\n","          inds[i] = np.argsort(d[i])[::-1]\n","          ranks[index + i] = np.where(inds[i] == index)[0][0]\n","          top1[index + i] = inds[i][0]\n","      \n","    #Compute metrics\n","    r1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\n","    r5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\n","    r10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)\n","    r100 = 100.0 * len(np.where(ranks < 100)[0]) / len(ranks)\n","    r1000 = 100.0 * len(np.where(ranks < 1000)[0]) / len(ranks)\n","    \n","    medr = np.floor(np.median(ranks)) + 1\n","    meanr = ranks.mean() + 1   \n","\n","    return (r1, r5, r10,r100,r1000, medr, meanr), (ranks, top1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bB-Z0ZoHxBnG"},"source":["(r1, r5, r10,r100, r1000,medr, meanr),(ranks,top1) = validate(test_data_loader,model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0op-jNiod4l1"},"source":["print(\"R@1: \"+str(round(r1,2))+\"%\")\n","print(\"R@5: \"+str(round(r5,2))+\"%\")\n","print(\"R@10: \"+str(round(r10,2))+\"%\")\n","print(\"R@100: \"+str(round(r100,2))+\"%\")\n","print(\"R@1000: \"+str(round(r1000,2))+\"%\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HJtCwUmoenxq"},"source":["# Train"]},{"cell_type":"code","metadata":{"id":"qimPHHXbu0ci"},"source":["import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ts3W2QcUlsYd"},"source":["#test = pd.read_csv('/content/drive/MyDrive/Code/PJDS/custom_MAXHAL/mediaeval_data.csv') #Full dataset\n","dataset_train = pd.read_csv('/content/drive/MyDrive/Code/PJDS/custom_MAXHAL/mediaeval_data_train_2.csv')\n","dataset_test = pd.read_csv('/content/drive/MyDrive/Code/PJDS/custom_MAXHAL/mediaeval_data_test_2.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-REyo5xDU81g"},"source":["datamodel_train = ArticleDataset(dataset_train,text_emb_model, split='train')\n","train_data_loader = torch.utils.data.DataLoader(dataset=datamodel_train,\n","                                              batch_size=128,\n","                                              shuffle=True,\n","                                              num_workers=4)\n","\n","datamodel_test = ArticleDataset(dataset_test,text_emb_model, split='test')\n","test_data_loader = torch.utils.data.DataLoader(dataset=datamodel_test,\n","                                              batch_size=300,\n","                                              shuffle=False,\n","                                              num_workers=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ryNpEB-egU8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627392611495,"user_tz":-120,"elapsed":4225,"user":{"displayName":"Tom Sühr","photoUrl":"","userId":"12788046760778604768"}},"outputId":"e438a591-cdee-44ae-8ebf-a9ee22f1dec8"},"source":["learning_rate = 0.002\n","model = MAXHAL(learning_rate)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["=> using pre-trained model 'vgg19'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7a9jHZcWj71m"},"source":["for epoch in range(30):\n","\n","  print(\"training for epoch \" + str(epoch))\n","  # if epoch in [20,40,60,80]:\n","  #   learning_rate = 0.002\n","  if epoch % 4  == 0 and epoch>0:\n","    print(\"LEARNING RATE DECREASE!\")\n","    learning_rate = learning_rate/2.0\n","    optimizer = model.optimizer\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = learning_rate\n","\n","  for i,train_data in enumerate(train_data_loader): \n","    model.train_start()\n","\n","    model.train_emb(*train_data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P107hcZf42u7"},"source":["#Mediaeval Evaluation"]},{"cell_type":"code","metadata":{"id":"-95_UjBaT7gv"},"source":["dataset_text = pd.read_csv('path/to/test/set')\n","categories = []\n","for url in dataset_text[\"url\"]:\n","  url_arr = url.split('/')\n","  cat = url_arr[len(url_arr)-2]\n","  categories.append(cat)\n","dataset_text[\"category\"] = categories"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uVvCtOe2QLly"},"source":["datamodel_text = TextMediaeval(dataset_text,text_emb_model)\n","text_data_loader = torch.utils.data.DataLoader(dataset=datamodel_text,\n","                                              batch_size=len(datamodel_text),\n","                                              shuffle=False,\n","                                              num_workers=2)\n","\n","datamodel_image = ImageMediaeval()\n","image_data_loader = torch.utils.data.DataLoader(dataset=datamodel_image,\n","                                              batch_size=100,\n","                                              shuffle=False,\n","                                              num_workers=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hbjQP6RoTBQf"},"source":["for i,train_data in enumerate(text_data_loader):\n","  model.val_start()\n","  text_embedding = model.txt_enc.forward(train_data[0].cuda(),train_data[1].cuda(),train_data[2].cuda())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WvOrOEE-kEoR"},"source":["text_embedding = text_embedding.detach().cpu().numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qMnBUH9xkK_B"},"source":["img_emb_list = []\n","for i,train_data in enumerate(image_data_loader):\n","  model.val_start() \n","  img_embedding = model.img_enc.forward(train_data[0].cuda())\n","  img_emb_list.append(img_embedding.detach().cpu().numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PA293tzG42F_"},"source":["vec = img_emb_list[0]\n","for arr in img_emb_list[1:]:\n","  vec = np.vstack((vec,arr))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M6TpFwKe5i3x"},"source":["vec.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PAlUDNIf5rxb"},"source":["img_embedding_full = vec"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sdQyJtI759Jy"},"source":["indexs_final = t2i_article_indexes(img_embedding_full, text_embedding)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ve5a6KAcZZp2"},"source":["len(np.unique(indexs_final))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NQKJT6BXRc34"},"source":["indexs_final = np.array(indexs_final)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fi3TghWeop0m"},"source":["img_list = [w.replace('.jpg', '') for w in datamodel_image.dataset]\n","dataset_text = dataset_text.astype({\"articleID\": int})\n","img_list = np.array(img_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A2U0aFJ3oV44"},"source":["csv_list = np.hstack((dataset_text['articleID'][0],img_list[indexs_final[0]]))\n","\n","for a,i in zip(dataset_text['articleID'][1:],indexs_final[1:]):\n","    csv_list = np.vstack((csv_list,np.hstack((a,img_list[i]))))\n","dd = pd.DataFrame(csv_list)\n","dd.to_csv('results.csv',index=False, sep ='\\t')"],"execution_count":null,"outputs":[]}]}